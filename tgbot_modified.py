import sys
import re
import os
import asyncio
import ast
from datetime import datetime
from typing import List, Dict, Set, Callable, Optional, Tuple
import time
from collections import deque, defaultdict
from pathlib import Path
from telegram.error import RetryAfter
from loguru import logger
from dotenv import load_dotenv
from functools import wraps
import aiohttp
from bs4 import BeautifulSoup
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.constants import ChatAction, ParseMode
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes, CallbackQueryHandler
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Bot version
BOT_VERSION = "END"

# ç•ªå·æ­£åˆ™è¡¨è¾¾å¼
FANHAO_REGEX = re.compile(
    r'^(?:[A-Za-z]{2,5}[-_ ]?\d{2,5}(?:[-_ ]?[A-Za-z]+)?|FC2-PPV-\d{6,})$', # ç¨å¾®è°ƒæ•´ FC2 åŒ¹é…
    re.IGNORECASE
)

# é…ç½®ç±»
class Config:
    def __init__(self):
        self.load_config()
    
    def load_config(self):
        """åŠ è½½æˆ–é‡æ–°åŠ è½½é…ç½®"""
        load_dotenv(override=True)
        self.TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN", "")
        self.BASE_URL = os.getenv("ALIST_BASE_URL", "")
        self.ALIST_TOKEN = os.getenv("ALIST_TOKEN", "")
        self.ALIST_OFFLINE_DIRS = [d.strip() for d in os.getenv("ALIST_OFFLINE_DIRS", "").split(",") if d.strip()]
        self.SEARCH_URLS = [url.strip() for url in os.getenv("JAV_SEARCH_APIS", "").split(",") if url.strip()]
        self.ALLOWED_USER_IDS = self._parse_allowed_user_ids(os.getenv("ALLOWED_USER_IDS", ""))
        self.CLEAN_INTERVAL_MINUTES = self._parse_int(os.getenv("CLEAN_INTERVAL_MINUTES", "60"), "CLEAN_INTERVAL_MINUTES", 1)
        self.SIZE_THRESHOLD = self._parse_int(os.getenv("SIZE_THRESHOLD", "100"), "SIZE_THRESHOLD", 0) * 1024 * 1024
        self.PREFERRED_KEYWORDS = [k.strip().lower() for k in os.getenv("PREFERRED_KEYWORDS", "").split(",") if k.strip()]
        self.CUSTOM_CATEGORIES = self._parse_custom_categories(os.getenv("CUSTOM_CATEGORIES", ""))
        self.SYSTEM_FOLDERS = self._parse_system_folders(os.getenv("SYSTEM_FOLDERS", ""))
        self.CLEAN_BATCH_SIZE = self._parse_int(os.getenv("CLEAN_BATCH_SIZE", "500"), "CLEAN_BATCH_SIZE", 1)
        self.CLEAN_REQUEST_INTERVAL = self._parse_float(os.getenv("CLEAN_REQUEST_INTERVAL", "0.2"), "CLEAN_REQUEST_INTERVAL", 0.1)
        self.MAX_CONCURRENT_REQUESTS = self._parse_int(os.getenv("MAX_CONCURRENT_REQUESTS", "20"), "MAX_CONCURRENT_REQUESTS", 1)
        self.EXCLUDE_SUFFIXES = [ext.strip().lower() for ext in os.getenv("EXCLUDE_SUFFIXES", "").split(",") if ext.strip()]

    def _parse_int(self, value: str, name: str, min_value: int) -> int:
        try:
            result = int(value)
            if result < min_value:
                logger.error(f"{name} å¿…é¡»å¤§äºæˆ–ç­‰äº {min_value}ï¼Œå½“å‰å€¼: {value}")
                sys.exit(1)
            return result
        except (ValueError, TypeError):
            logger.error(f"{name} æ ¼å¼ä¸æ­£ç¡®ï¼Œå¿…é¡»ä¸ºæ•´æ•°ï¼Œå½“å‰å€¼: {value}")
            sys.exit(1)

    def _parse_float(self, value: str, name: str, min_value: float) -> float:
        try:
            result = float(value)
            if result < min_value:
                logger.error(f"{name} å¿…é¡»å¤§äºæˆ–ç­‰äº {min_value}ï¼Œå½“å‰å€¼: {value}")
                sys.exit(1)
            return result
        except (ValueError, TypeError):
            logger.error(f"{name} æ ¼å¼ä¸æ­£ç¡®ï¼Œå¿…é¡»ä¸ºæµ®ç‚¹æ•°ï¼Œå½“å‰å€¼: {value}")
            sys.exit(1)

    def _parse_allowed_user_ids(self, user_ids_str: str) -> Set[int]:
        if not user_ids_str.strip():
            logger.error("ALLOWED_USER_IDS æœªæä¾›")
            sys.exit(1)
        try:
            return set(map(int, user_ids_str.split(',')))
        except (ValueError, AttributeError):
            logger.error(f"ALLOWED_USER_IDS æ ¼å¼ä¸æ­£ç¡®: {user_ids_str}")
            sys.exit(1)

    def _parse_custom_categories(self, categories_str: str) -> List[Dict[str, List[str]]]:
        if not categories_str.strip():
            logger.debug("CUSTOM_CATEGORIES æœªé…ç½®ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»")
            return []
        categories = []
        try:
            for category in categories_str.split(';'):
                if not category.strip():
                    continue
                parts = category.split(':')
                if len(parts) != 2:
                    logger.warning(f"æ— æ•ˆåˆ†ç±»é…ç½®ï¼ˆç¼ºå°‘åç§°æˆ–å…³é”®è¯ï¼‰: {category}")
                    continue
                name, keywords = parts
                name = name.strip()
                keywords = [kw.strip().lower() for kw in keywords.split(',') if kw.strip()]
                if not name or not keywords:
                    logger.warning(f"æ— æ•ˆåˆ†ç±»é…ç½®ï¼ˆç©ºåç§°æˆ–å…³é”®è¯ï¼‰: {category}")
                    continue
                categories.append({"name": name, "keywords": keywords})
            logger.info(f"åŠ è½½ {len(categories)} ä¸ªè‡ªå®šä¹‰åˆ†ç±»")
            return categories
        except Exception as e:
            logger.error(f"CUSTOM_CATEGORIES è§£æé”™è¯¯: {categories_str}, é”™è¯¯: {str(e)}")
            return []

    def _parse_system_folders(self, folders_str: str) -> List[str]:
        if not folders_str.strip():
            logger.debug("SYSTEM_FOLDERS æœªé…ç½®ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        try:
            folders = [f.strip() for f in folders_str.split(',') if f.strip()]
            normalized_folders = []
            for folder in folders:
                folder = folder.strip().replace('\\', '/')
                if not folder.startswith('/'):
                    folder = f"/{folder}"
                folder = folder.rstrip('/')
                if folder:
                    normalized_folders.append(folder)
            logger.info(f"åŠ è½½ {len(normalized_folders)} ä¸ªç³»ç»Ÿæ–‡ä»¶å¤¹")
            return normalized_folders
        except Exception as e:
            logger.error(f"SYSTEM_FOLDERS è§£æé”™è¯¯: {folders_str}, é”™è¯¯: {str(e)}")
            return []

    def validate(self):
        required = [
            (self.TELEGRAM_TOKEN, "TELEGRAM_TOKEN"),
            (self.BASE_URL, "ALIST_BASE_URL"),
            (self.ALIST_TOKEN, "ALIST_TOKEN"),
            (self.ALIST_OFFLINE_DIRS, "ALIST_OFFLINE_DIRS"),
            (self.SEARCH_URLS, "JAV_SEARCH_APIS"),
            (self.ALLOWED_USER_IDS, "ALLOWED_USER_IDS")
        ]
        for value, name in required:
            if not value:
                logger.error(f"ç¯å¢ƒå˜é‡ {name} ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
                return False
        return True

# åˆå§‹åŒ–é…ç½®
config = Config()
if not config.validate():
    sys.exit(1)

# é…ç½® Loguru æ—¥å¿—
logger.remove()
logger.add("bot.log", rotation="10 MB", level="INFO", filter=lambda record: "Authorization" not in record["message"])
logger.add(sys.stderr, level="INFO", filter=lambda record: "Authorization" not in record["message"])

# ç”¨æˆ·æˆæƒè£…é¥°å™¨
def restricted(func):
    @wraps(func)
    async def wrapped(update: Update, context: ContextTypes.DEFAULT_TYPE, *args, **kwargs):
        user_id = update.effective_user.id
        if user_id not in config.ALLOWED_USER_IDS:
            await update.effective_message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
            return
        # å¦‚æœå‡½æ•°ç­¾ååŒ…å« token å‚æ•°ï¼Œåˆ™ä¼ é€’ï¼Œå¦åˆ™ä¸ä¼ é€’
        if 'token' in func.__code__.co_varnames:
            return await func(update, context, token=config.ALIST_TOKEN, *args, **kwargs)
        return await func(update, context, *args, **kwargs)
    return wrapped

# å·¥å…·å‡½æ•°
class AsyncLimiter:
    """å¼‚æ­¥é€Ÿç‡é™åˆ¶å™¨ï¼Œç”¨äºæ§åˆ¶APIè¯·æ±‚é¢‘ç‡"""
    def __init__(self, rate_limit: int, time_window: float = 1.0):
        self.rate_limit = rate_limit
        self.time_window = time_window
        self.times = deque()
        self.lock = asyncio.Lock()

    async def __aenter__(self):
        async with self.lock:
            now = time.time()
            while self.times and now - self.times[0] >= self.time_window:
                self.times.popleft()
            if len(self.times) >= self.rate_limit:
                sleep_time = self.time_window - (now - self.times[0])
                await asyncio.sleep(sleep_time)
                self.times.popleft()
            self.times.append(now)

    async def __aexit__(self, exc_type, exc, tb):
        pass

def normalize_path(path: str) -> str:
    """æ ‡å‡†åŒ–è·¯å¾„æ ¼å¼ï¼ˆå…¼å®¹Windows/Unixï¼‰"""
    return str(Path(path).resolve()).replace('\\', '/')

def is_system_path(path: str, system_folders: List[str]) -> bool:
    """æ£€æŸ¥è·¯å¾„æ˜¯å¦ä¸ºç³»ç»Ÿæ–‡ä»¶å¤¹"""
    path = path.lower().rstrip('/')
    return any(
        path == sf.lower().rstrip('/') or 
        path.startswith(f"{sf.lower().rstrip('/')}/")
        for sf in system_folders
    )

def normalize_fanhao(fanhao: str) -> str:
    """æ ‡å‡†åŒ–ç•ªå·ï¼Œç§»é™¤åˆ†éš”ç¬¦å¹¶è½¬æ¢ä¸ºå°å†™"""
    return re.sub(r'[-_ ]', '', fanhao).lower()

def is_fanhao_match(fanhao: str, name: str) -> bool:
    """æ£€æŸ¥åç§°æ˜¯å¦åŒ…å«æŒ‡å®šçš„ç•ªå·ï¼ˆæ”¯æŒå˜ä½“ï¼‰"""
    # æ ‡å‡†åŒ–ç•ªå·ï¼šç§»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦å¹¶è½¬ä¸ºå°å†™
    normalized_fanhao = re.sub(r'[^a-z0-9]', '', fanhao.lower())
    # å¤„ç†åç§°ï¼šç§»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦å¹¶è½¬ä¸ºå°å†™
    normalized_name = re.sub(r'[^a-z0-9]', '', name.lower())
    
    # æ£€æŸ¥æ ‡å‡†åŒ–åçš„ç•ªå·æ˜¯å¦å­˜åœ¨äºå¤„ç†åçš„åç§°ä¸­
    result = normalized_fanhao in normalized_name
    
    logger.debug(f"æ ‡å‡†åŒ–ç•ªå·: {normalized_fanhao}, æ ‡å‡†åŒ–åç§°: {normalized_name}, åŒ¹é…ç»“æœ: {result}")
    
    if not result:
        logger.debug(f"åŒ¹é…å¤±è´¥: ç•ªå· {fanhao} æœªåœ¨åç§° {name} ä¸­æ‰¾åˆ°")
    
    return result

def parse_size_to_bytes(size_str: str) -> int:
    """å°†å¤§å°å­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—èŠ‚"""
    try:
        size_str = size_str.lower().strip()
        logger.debug(f"è§£æå¤§å°å­—ç¬¦ä¸²: {size_str}")
        if not size_str:
            logger.warning("å¤§å°å­—ç¬¦ä¸²ä¸ºç©º")
            return 0
        units = {'kb': 1024, 'mb': 1024**2, 'gb': 1024**3, 'tb': 1024**4}
        # åŒ¹é…æ•°å­—+å•ä½ï¼Œå¦‚ "1.2 GB", "500 MB"
        match = re.match(r'(\d*\.?\d+)\s*([kmgt]b)', size_str)
        if not match:
            logger.warning(f"æ— æ³•è§£æå¤§å°å­—ç¬¦ä¸²: {size_str}")
            return 0
        value, unit = float(match.group(1)), match.group(2)
        size_bytes = int(value * units[unit])
        logger.debug(f"è§£æç»“æœ: {size_bytes} å­—èŠ‚")
        return size_bytes
    except (ValueError, KeyError) as e:
        logger.warning(f"å¤§å°è§£æé”™è¯¯: {size_str}, å¼‚å¸¸: {str(e)}")
        return 0

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(aiohttp.ClientError)
)
async def fetch_api(url: str, session: aiohttp.ClientSession, fanhao: str, headers: Dict) -> List[Dict]:
    """ä»APIæœç´¢ç£åŠ›é“¾æ¥ï¼Œç¡®ä¿ç»“æœä¸ç•ªå·åŒ¹é…"""
    entries = []
    if not url.strip():
        return entries
    try:
        async with session.get(f"{url.rstrip('/')}/{fanhao}", headers=headers, timeout=20) as response:
            response.raise_for_status()
            raw_result = await response.json()
            if not raw_result or raw_result.get("status") != "succeed":
                logger.debug(f"API {url} è¿”å›ç©ºæˆ–å¤±è´¥çŠ¶æ€")
                return entries
            total_results = len(raw_result.get("data", []))
            for entry_str in raw_result.get("data", []):
                try:
                    data = ast.literal_eval(entry_str)
                    if not isinstance(data, list) or len(data) < 4:
                        continue
                    magnet, name, size_str, date_str = data[:4]
                    if not magnet.startswith("magnet:?"):
                        continue
                    logger.debug(f"API ç»“æœåç§°: {name}")
                    if not is_fanhao_match(fanhao, name):
                        logger.debug(f"API ç»“æœå¿½ç•¥: {name} ä¸åŒ¹é…ç•ªå· {fanhao}")
                        continue
                    size_bytes = parse_size_to_bytes(size_str) if size_str else 0
                    if not size_bytes:
                        continue
                    upload_date = None
                    try:
                        if date_str:
                            upload_date = datetime.strptime(date_str, '%Y-%m-%d').date()
                    except ValueError:
                        pass
                    entries.append({
                        "magnet": magnet,
                        "name": name,
                        "size_bytes": size_bytes,
                        "date": upload_date,
                        "seeders": 0,
                        "leechers": 0,
                        "source": f"API ({url})"
                    })
                except (ValueError, SyntaxError, TypeError) as e:
                    logger.debug(f"API ç»“æœè§£æé”™è¯¯: {entry_str}, é”™è¯¯: {str(e)}")
            logger.debug(f"API {url} æ€»è¿”å› {total_results} æ¡ï¼ŒåŒ¹é… {len(entries)} æ¡")
            return entries
    except aiohttp.ClientError as e:
        logger.warning(f"API æœç´¢å¤±è´¥: {url}: {str(e)}")
        return entries

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(aiohttp.ClientError)
)
async def scrape_sukebei(session: aiohttp.ClientSession, fanhao: str, headers: Dict) -> List[Dict]:
    """ä»sukebei.nyaa.siæŠ“å–ç£åŠ›é“¾æ¥ï¼Œç¡®ä¿ç»“æœä¸ç•ªå·åŒ¹é…"""
    entries = []
    try:
        url = f"https://sukebei.nyaa.si/?f=0&c=0_0&q={fanhao}"
        async with session.get(url, headers=headers, timeout=20) as response:
            response.raise_for_status()
            text = await response.text()
            soup = BeautifulSoup(text, 'html.parser')
            torrent_rows = soup.select('tr.default, tr.success')
            total_results = len(torrent_rows)
            matched_count = 0
            for tr in torrent_rows:
                name_elem = tr.select_one('td:nth-child(2) a')
                name = name_elem.text.strip() if name_elem else "æœªçŸ¥"
                logger.debug(f"sukebei ç»“æœåç§°: {name}")
                if not is_fanhao_match(fanhao, name):
                    logger.debug(f"sukebei ç»“æœå¿½ç•¥: {name} ä¸åŒ¹é…ç•ªå· {fanhao}")
                    continue
                magnet_a = tr.find('a', href=lambda x: x and x.startswith('magnet:?'))
                if not magnet_a:
                    logger.debug(f"sukebei ç»“æœå¿½ç•¥: {name} æ— ç£åŠ›é“¾æ¥")
                    continue
                magnet = magnet_a['href']
                size_td = tr.select_one('td:nth-child(3)')
                size_str = size_td.text.strip() if size_td else ""
                size_bytes = parse_size_to_bytes(size_str) if size_str else 0
                # æ”¾å®½è¿‡æ»¤æ¡ä»¶ï¼Œå…è®¸ size_bytes ä¸º 0
                if size_bytes == 0:
                    logger.warning(f"èµ„æº {name} å¤§å°ä¸º 0ï¼Œå¯èƒ½è§£æå¤±è´¥")
                date_td = tr.select_one('td:nth-child(5)[data-timestamp]')
                date_str = date_td.text.strip() if date_td else ""
                upload_date = None
                try:
                    if date_str:
                        upload_date = datetime.strptime(date_str, '%Y-%m-%d %H:%M').date()
                except ValueError:
                    pass
                seeders = leechers = 0
                tds = tr.select('td.text-center')
                if len(tds) >= 5:
                    try:
                        seeders = int(tds[2].text.strip())
                        leechers = int(tds[3].text.strip())
                    except ValueError:
                        pass
                entries.append({
                    "magnet": magnet,
                    "name": name,
                    "size_bytes": size_bytes,
                    "date": upload_date,
                    "seeders": seeders,
                    "leechers": leechers,
                    "source": "sukebei"
                })
                matched_count += 1
            logger.debug(f"sukebei.nyaa.si æ€»è¿”å› {total_results} æ¡ï¼ŒåŒ¹é… {matched_count} æ¡")
            return entries
    except aiohttp.ClientError as e:
        logger.warning(f"sukebei.nyaa.si æœç´¢å¤±è´¥: {str(e)}")
        return entries

async def search_magnet(fanhao: str, search_urls: List[str], context: ContextTypes.DEFAULT_TYPE) -> Tuple[Optional[str], Optional[str]]:
    """æœç´¢ç£åŠ›é“¾æ¥å¹¶é€‰æ‹©æœ€ä½³ç»“æœ"""
    if not FANHAO_REGEX.match(fanhao):
        return None, f"âŒ æ— æ•ˆç•ªå·æ ¼å¼: {fanhao}"
    
    async with aiohttp.ClientSession() as session:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        logger.info(f"å¼€å§‹å¹¶è¡Œæœç´¢ç•ªå·: {fanhao}")
        api_tasks = [fetch_api(url, session, fanhao, headers) for url in search_urls if url.strip()]
        results = await asyncio.gather(*api_tasks, scrape_sukebei(session, fanhao, headers), return_exceptions=True)
        all_entries = []
        api_results_count = 0
        sukebei_results_count = 0
        failed_sources = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                failed_sources += 1
                logger.warning(f"æœç´¢æº {search_urls[i] if i < len(api_tasks) else 'sukebei.nyaa.si'} å¤±è´¥: {str(result)}")
                continue
            if i < len(api_tasks):
                api_results_count += len(result)
                logger.debug(f"API {search_urls[i]} è¿”å› {len(result)} æ¡åŒ¹é…ç»“æœ")
            else:
                sukebei_results_count = len(result)
                logger.debug(f"sukebei.nyaa.si è¿”å› {len(result)} æ¡åŒ¹é…ç»“æœ")
            logger.debug(f"æ·»åŠ ç»“æœ: {len(result)} æ¡ï¼Œæ¥æº: {'API' if i < len(api_tasks) else 'sukebei'}")
            all_entries.extend(result)
        
        logger.info(f"æœç´¢ç»“æœç»Ÿè®¡: API æ€»è®¡ {api_results_count} æ¡, sukebei.nyaa.si {sukebei_results_count} æ¡")
        
        if not all_entries:
            error_msg = f"ğŸ” æœªæ‰¾åˆ°ç•ªå· {fanhao} çš„ç›¸å…³èµ„æºï¼ˆAPI: {api_results_count} æ¡åŒ¹é…, sukebei: {sukebei_results_count} æ¡åŒ¹é…ï¼‰"
            if failed_sources == len(api_tasks) + 1:
                error_msg += f"\nâš ï¸ æ‰€æœ‰ {failed_sources} ä¸ªæœç´¢æºå‡å¤±è´¥"
            return None, error_msg
        
        seen_magnets = set()
        unique_entries = []
        for entry in all_entries:
            magnet = entry["magnet"]
            if magnet not in seen_magnets:
                seen_magnets.add(magnet)
                unique_entries.append(entry)
        logger.info(f"å»é‡åå‰©ä½™ {len(unique_entries)} æ¡ç»“æœ")
        
        if not unique_entries:
            return None, f"ğŸ” æœªæ‰¾åˆ°æœ‰æ•ˆç£åŠ›é“¾æ¥ï¼ˆAPI: {api_results_count} æ¡, sukebei: {sukebei_results_count} æ¡ï¼‰"
        
        # ä¸è¿‡åº¦è¿‡æ»¤ size_bytes ä¸º 0 çš„æ¡ç›®
        valid_entries = unique_entries
        logger.debug(f"æœ‰æ•ˆæ¡ç›®: {len(valid_entries)} æ¡")
        if not valid_entries:
            return None, f"âŒ æœªæ‰¾åˆ°ä¸ç•ªå· {fanhao} åŒ¹é…çš„æœ‰æ•ˆç£åŠ›é“¾æ¥ï¼ˆAPI: {api_results_count} æ¡, sukebei: {sukebei_results_count} æ¡ï¼‰"
        
        if len(valid_entries) == 1:
            selected_entry = valid_entries[0]
            logger.info(f"ä»…ä¸€æ¡æœ‰æ•ˆç»“æœï¼Œé€‰æ‹©æ¥æº: {selected_entry['source']}, åç§°: {selected_entry['name'][:50]}...")
            return selected_entry["magnet"], None
        
        def has_preferred_keyword(entry):
            if not config.PREFERRED_KEYWORDS:
                return 0
            name_lower = entry["name"].lower()
            return any(keyword in name_lower for keyword in config.PREFERRED_KEYWORDS)
        
        # ä¼˜å…ˆé€‰æ‹©æœ‰ç§å­çš„æ¡ç›®
        selected = sorted(
            valid_entries,
            key=lambda x: (
                has_preferred_keyword(x),
                x.get("seeders", 0),
                x["date"].toordinal() if x["date"] else float('-inf'),
                x["size_bytes"]
            ),
            reverse=True
        )
        selected_entry = selected[0]
        logger.info(f"é€‰æ‹©æœ€ä½³ç£åŠ›é“¾æ¥ï¼Œæ¥æº: {selected_entry['source']}, åç§°: {selected_entry['name'][:50]}...")
        return selected_entry["magnet"], None

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, RetryAfter))
)
async def add_offline_download(context: ContextTypes.DEFAULT_TYPE, token: str, links: List[str]) -> Tuple[bool, str]:
    """æ·»åŠ ç¦»çº¿ä¸‹è½½ä»»åŠ¡åˆ°Alist"""
    if not token or not links:
        return False, "âŒ ç¼ºå°‘å¿…è¦å‚æ•°"
    try:
        url = f"{config.BASE_URL.rstrip('/')}/api/fs/add_offline_download"
        headers = {"Authorization": token, "Content-Type": "application/json"}
        post_data = {
            "path": context.bot_data.get('current_download_dir', config.ALIST_OFFLINE_DIRS[0]),
            "urls": links,
            "tool": "storage",
            "delete_policy": "delete_on_upload_succeed"
        }
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=post_data, headers=headers, timeout=30) as response:
                status = response.status
                result = await response.json()
                if status == 401:
                    return False, "âŒ è®¤è¯å¤±è´¥"
                if status == 500:
                    return False, "âŒ æœåŠ¡å™¨æ‹’ç»è¯·æ±‚"
                if status == 200 and result.get("code") == 200:
                    return True, "âœ… å·²æ·»åŠ åˆ°ä¸‹è½½é˜Ÿåˆ—"
                return False, f"âŒ è§£æå¤±è´¥"
    except aiohttp.ClientError as e:
        logger.error(f"æ·»åŠ ä»»åŠ¡å¤±è´¥: {str(e)}")
        raise

async def recursive_collect_files(token: str, base_url: str, root_dir: str) -> Tuple[Dict[str, List[str]], Set[str]]:
    """é€’å½’æ”¶é›†å°æ–‡ä»¶å¹¶åˆ†ç»„"""
    dir_files = defaultdict(list)
    known_empty_dirs = set()

    async def collect_files_worker(dir_path: str, session: aiohttp.ClientSession):
        list_url = f"{base_url.rstrip('/')}/api/fs/list"
        headers = {"Authorization": token}
        payload = {"path": dir_path, "page": 1, "per_page": 1000, "refresh": True}
        try:
            async with session.post(list_url, json=payload, headers=headers) as resp:
                if resp.status != 200:
                    logger.error(f"åˆ—å‡ºç›®å½• {dir_path} å¤±è´¥: çŠ¶æ€ç  {resp.status}")
                    return
                data = await resp.json()
                if data.get("code") != 200:
                    logger.error(f"åˆ—å‡ºç›®å½• {dir_path} å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return
                content = data.get("data", {}).get("content")
                if content is None:
                    known_empty_dirs.add(dir_path)
                    return

                tasks = []
                for item in content:
                    item_path = os.path.join(dir_path, item["name"]).replace("\\", "/")
                    if item["is_dir"]:
                        tasks.append(collect_files_worker(item_path, session))
                    elif (
                        item["size"] < config.SIZE_THRESHOLD and
                        not any(item["name"].lower().endswith(ext) for ext in config.EXCLUDE_SUFFIXES)
                    ):
                        dir_files[dir_path].append(item["name"])
                if tasks:
                    await asyncio.gather(*tasks, return_exceptions=True)
        except aiohttp.ClientError as e:
            logger.error(f"åˆ—å‡ºç›®å½• {dir_path} å¤±è´¥: {str(e)}")

    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
        await collect_files_worker(root_dir, session)
    return dict(dir_files), known_empty_dirs

async def recursive_collect_empty_dirs(token: str, base_url: str, root_dir: str) -> List[str]:
    """é€’å½’æ”¶é›†ç©ºæ–‡ä»¶å¤¹"""
    empty_dirs = set()

    async def check_dir(dir_path: str, session: aiohttp.ClientSession):
        if is_system_path(dir_path, config.SYSTEM_FOLDERS):
            return
        list_url = f"{base_url.rstrip('/')}/api/fs/list"
        headers = {"Authorization": token}
        payload = {"path": dir_path, "page": 1, "per_page": 1000, "refresh": True}
        try:
            async with session.post(list_url, json=payload, headers=headers) as resp:
                if resp.status != 200:
                    return
                data = await resp.json()
                if data.get("code") != 200:
                    return
                content = data.get("data", {}).get("content")
                if content is None or len(content) == 0:
                    empty_dirs.add(dir_path)
                    return
                tasks = []
                for item in content:
                    if item["is_dir"]:
                        item_path = os.path.join(dir_path, item["name"]).replace("\\", "/")
                        tasks.append(check_dir(item_path, session))
                if tasks:
                    await asyncio.gather(*tasks, return_exceptions=True)
        except aiohttp.ClientError as e:
            logger.error(f"åˆ—å‡ºç›®å½• {dir_path} å¤±è´¥: {str(e)}")

    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
        await check_dir(root_dir, session)
    return list(empty_dirs)

async def cleanup_small_files(token: str, base_url: str, root_dir: str, progress_callback: Callable[[int], None] = None) -> Tuple[int, str]:
    """æ¸…ç†å°æ–‡ä»¶ï¼Œæ”¯æŒå¹¶è¡Œåˆ é™¤"""
    start_time = datetime.now()
    list_url = f"{base_url.rstrip('/')}/api/fs/list"
    headers = {"Authorization": token}
    payload = {"path": root_dir, "page": 1, "per_page": 0}
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
            async with session.post(list_url, json=payload, headers=headers) as resp:
                if resp.status != 200:
                    return 0, f"âŒ æ— æ³•è®¿é—®è·¯å¾„ {root_dir}: çŠ¶æ€ç  {resp.status}"
                data = await resp.json()
                if data.get("code") != 200:
                    return 0, f"âŒ è·¯å¾„æ— æ•ˆ: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"
    except aiohttp.ClientError as e:
        return 0, f"âŒ è·¯å¾„è®¿é—®å¤±è´¥: {str(e)}"

    dir_files, _ = await recursive_collect_files(token, base_url, root_dir)
    total_files = sum(len(files) for files in dir_files.values())
    logger.info(f"å¼€å§‹æ¸…ç† {root_dir}ï¼Œå‘ç° {total_files} ä¸ªå°æ–‡ä»¶")

    merged_batches = []
    current_batch = []
    current_batch_size = 0
    for parent, names in dir_files.items():
        if is_system_path(parent, config.SYSTEM_FOLDERS):
            continue
        for name in names:
            if current_batch_size < config.CLEAN_BATCH_SIZE:
                current_batch.append((parent, name))
                current_batch_size += 1
            else:
                merged_batches.append(current_batch)
                current_batch = [(parent, name)]
                current_batch_size = 1
    if current_batch:
        merged_batches.append(current_batch)

    deleted_count = 0
    errors = []
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=config.MAX_CONCURRENT_REQUESTS)) as session:
        for batch_idx, batch in enumerate(merged_batches, 1):
            batch_by_parent = defaultdict(list)
            for parent, name in batch:
                batch_by_parent[parent].append(name)

            async def delete_batch(parent: str, names: List[str], retry_count: int = 0) -> Tuple[int, str]:
                max_retries = 5
                base_delay = 1
                try:
                    remove_url = f"{base_url.rstrip('/')}/api/fs/remove"
                    headers = {"Authorization": token}
                    data = {"dir": parent, "names": names}
                    async with session.post(remove_url, json=data, headers=headers) as resp:
                        if resp.status == 429:
                            if retry_count < max_retries:
                                delay = base_delay * (2 ** retry_count)
                                logger.warning(f"API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {delay} ç§’")
                                await asyncio.sleep(delay)
                                return await delete_batch(parent, names, retry_count + 1)
                            return 0, f"{parent}: é€Ÿç‡é™åˆ¶"
                        result = await resp.json()
                        if result.get("code") == 200:
                            if progress_callback:
                                progress_callback(len(names))
                            return len(names), ""
                        return 0, f"{parent}: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
                except aiohttp.ClientError as e:
                    return 0, f"{parent}: ç½‘ç»œé”™è¯¯ - {str(e)}"

            tasks = [delete_batch(parent, names) for parent, names in batch_by_parent.items()]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for (count, error), (parent, names) in zip(results, batch_by_parent.items()):
                if isinstance(count, Exception):
                    errors.append(f"{parent}: ä»»åŠ¡å¤±è´¥ - {str(count)}")
                    continue
                deleted_count += count
                if error:
                    errors.append(error)

            await asyncio.sleep(config.CLEAN_REQUEST_INTERVAL)

    elapsed = (datetime.now() - start_time).total_seconds()
    logger.info(f"æ¸…ç† {root_dir} å®Œæˆï¼Œ{deleted_count}/{total_files} æ–‡ä»¶ï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
    if errors:
        return deleted_count, (
            f"âš ï¸ éƒ¨åˆ†æ¸…ç†å¤±è´¥ï¼ˆæˆåŠŸ {deleted_count} æ–‡ä»¶ï¼‰\n" +
            "\n".join([f"â€¢ {msg}" for msg in errors[:3]])
        )
    return deleted_count, f"âœ… æˆåŠŸæ¸…ç† {deleted_count} ä¸ªå°æ–‡ä»¶"

async def cleanup_empty_dirs(token: str, base_url: str, root_dir: str, 
                          progress_callback: Callable[[int], None] = None) -> Tuple[int, str]:
    """æ¸…ç†ç©ºæ–‡ä»¶å¤¹"""
    start_time = datetime.now()
    list_url = f"{base_url.rstrip('/')}/api/fs/list"
    headers = {"Authorization": token}
    payload = {"path": root_dir, "page": 1, "per_page": 0}
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
            async with session.post(list_url, json=payload, headers=headers) as resp:
                if resp.status != 200:
                    return 0, f"âŒ æ— æ³•è®¿é—®è·¯å¾„ {root_dir}: çŠ¶æ€ç  {resp.status}"
                data = await resp.json()
                if data.get("code") != 200:
                    return 0, f"âŒ è·¯å¾„æ— æ•ˆ: {data.get('message', 'æœªçŸ¥é”™è¯¯')}"
    except aiohttp.ClientError as e:
        return 0, f"âŒ è·¯å¾„è®¿é—®å¤±è´¥: {str(e)}"

    empty_dirs = await recursive_collect_empty_dirs(token, base_url, root_dir)
    if not empty_dirs:
        logger.info(f"æœªæ‰¾åˆ°ç©ºæ–‡ä»¶å¤¹: {root_dir}")
        return 0, "âœ… æœªæ‰¾åˆ°ç©ºæ–‡ä»¶å¤¹"

    total_deleted = 0
    error_messages = []
    merged_groups = defaultdict(list)
    for dir_path in empty_dirs:
        if is_system_path(dir_path, config.SYSTEM_FOLDERS):
            continue
        parent = os.path.dirname(dir_path)
        name = os.path.basename(dir_path)
        merged_groups[parent].append(name)

    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=config.MAX_CONCURRENT_REQUESTS)) as session:
        async def delete_empty_batch(parent: str, names: List[str], retry_count: int = 0) -> Tuple[int, str]:
            max_retries = 5
            base_delay = 1
            try:
                remove_url = f"{base_url.rstrip('/')}/api/fs/remove"
                headers = {"Authorization": token}
                data = {"dir": parent, "names": names}
                async with session.post(remove_url, json=data, headers=headers) as resp:
                    if resp.status == 429:
                        if retry_count < max_retries:
                            delay = base_delay * (2 ** retry_count)
                            logger.warning(f"API é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {delay} ç§’")
                            await asyncio.sleep(delay)
                            return await delete_empty_batch(parent, names, retry_count + 1)
                        return 0, f"{parent}: é€Ÿç‡é™åˆ¶"
                    result = await resp.json()
                    if result.get("code") == 200:
                        count = len(names)
                        if progress_callback:
                            progress_callback(count)
                        return count, ""
                    return 0, f"{parent}: {result.get('message', 'æœªçŸ¥é”™è¯¯')}"
            except aiohttp.ClientError as e:
                return 0, f"{parent}: ç½‘ç»œé”™è¯¯ - {str(e)}"

        tasks = [delete_empty_batch(parent, names) for parent, names in merged_groups.items()]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for (count, error), (parent, names) in zip(results, merged_groups.items()):
            if isinstance(count, Exception):
                error_messages.append(f"{parent}: ä»»åŠ¡å¤±è´¥ - {str(count)}")
                continue
            total_deleted += count
            if error:
                error_messages.append(error)

    elapsed = (datetime.now() - start_time).total_seconds()
    logger.info(f"æ¸…ç†ç©ºæ–‡ä»¶å¤¹ {root_dir} å®Œæˆï¼Œ{total_deleted} ä¸ªï¼Œè€—æ—¶ {elapsed:.2f} ç§’")
    if error_messages:
        return total_deleted, (
            f"âš ï¸ éƒ¨åˆ†åˆ é™¤å¤±è´¥ï¼ˆæˆåŠŸ {total_deleted} æ–‡ä»¶å¤¹ï¼‰\n" +
            "\n".join([f"â€¢ {msg}" for msg in error_messages[:3]])
        )
    return total_deleted, f"âœ… æˆåŠŸåˆ é™¤ {total_deleted} ä¸ªç©ºæ–‡ä»¶å¤¹"

async def auto_clean(context: ContextTypes.DEFAULT_TYPE) -> None:
    """è‡ªåŠ¨æ¸…ç†ä»»åŠ¡"""
    token = config.ALIST_TOKEN
    base_url = config.BASE_URL
    results = []
    start_msg = (f"ğŸ”„ è‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¯åŠ¨\n"
                 f"â€¢ æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    # ä»…å‘å¯ç”¨æ¸…ç†é€šçŸ¥çš„ç”¨æˆ·å‘é€æ¶ˆæ¯
    for user_id in config.ALLOWED_USER_IDS:
        if context.bot_data.get(f"notify_clean_{user_id}", False):
            logger.info(f"Sending auto-clean start message to user {user_id}")
            try:
                await context.bot.send_message(
                    chat_id=user_id,
                    text=start_msg,
                    parse_mode=ParseMode.MARKDOWN
                )
                await asyncio.sleep(0.3)
            except aiohttp.ClientError as e:
                logger.error(f"å‘é€æ¶ˆæ¯åˆ° {user_id} å¤±è´¥: {str(e)}")

    # è¿‡æ»¤æ‰å­ç›®å½•ï¼Œåªæ¸…ç†é¡¶å±‚ç›®å½•
    unique_dirs = []
    for dir_path in sorted(config.ALIST_OFFLINE_DIRS, key=len):
        if not any(dir_path.startswith(parent + '/') for parent in unique_dirs):
            unique_dirs.append(dir_path)

    try:
        for target_dir in unique_dirs:
            dir_msg = []
            try:
                # éªŒè¯ç›®å½•å­˜åœ¨
                async with aiohttp.ClientSession() as session:
                    list_url = f"{base_url.rstrip('/')}/api/fs/list"
                    headers = {"Authorization": token}
                    payload = {"path": target_dir, "page": 1, "per_page": 0}
                    async with session.post(list_url, json=payload, headers=headers) as resp:
                        if resp.status != 200:
                            results.append(f"âŒ {target_dir} ä¸å¯è®¿é—®: çŠ¶æ€ç  {resp.status}")
                            continue
                        data = await resp.json()
                        if data.get("code") != 200:
                            results.append(f"âŒ {target_dir} æ— æ•ˆ: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                            continue

                deleted_files, msg_files = await cleanup_small_files(token, base_url, target_dir)
                dir_msg.append(f"å°æ–‡ä»¶: {msg_files}")
                deleted_dirs, msg_dirs = await cleanup_empty_dirs(token, base_url, target_dir)
                dir_msg.append(f"ç©ºç›®å½•: {msg_dirs}")
                results.append(f"ğŸ“‚ ç›®å½• {target_dir}:\n" + "\n".join(dir_msg))
            except aiohttp.ClientError as e:
                error_detail = f"HTTP é”™è¯¯: {str(e)}"
                try:
                    async with aiohttp.ClientSession() as session:
                        async with session.post(list_url, json=payload, headers=headers) as resp:
                            error_detail += f", çŠ¶æ€ç : {resp.status}, å“åº”: {await resp.text()[:100]}"
                except Exception as inner_e:
                    error_detail += f", æ— æ³•è·å–å“åº”: {str(inner_e)}"
                results.append(f"âŒ {target_dir} æ¸…ç†å¤±è´¥: {error_detail}")
            await asyncio.sleep(1)  # é¿å… API é€Ÿç‡é™åˆ¶

        summary = [
            f"âœ… è‡ªåŠ¨æ¸…ç†å®Œæˆ",
            f"â€¢ æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        ] + results
        max_length = 4000
        current_msg = []
        for line in summary:
            if len('\n'.join(current_msg + [line])) > max_length:
                message = '\n'.join(current_msg)
                for user_id in config.ALLOWED_USER_IDS:
                    if context.bot_data.get(f"notify_clean_{user_id}", False):
                        logger.info(f"Sending auto-clean summary message to user {user_id}")
                        try:
                            await context.bot.send_message(
                                chat_id=user_id,
                                text=message,
                                parse_mode=ParseMode.MARKDOWN
                            )
                            await asyncio.sleep(0.3)
                        except aiohttp.ClientError as e:
                            logger.error(f"å‘é€æ¶ˆæ¯åˆ° {user_id} å¤±è´¥: {str(e)}")
                current_msg = [line]
            else:
                current_msg.append(line)
        if current_msg:
            message = '\n'.join(current_msg)
            for user_id in config.ALLOWED_USER_IDS:
                if context.bot_data.get(f"notify_clean_{user_id}", False):
                    logger.info(f"Sending final auto-clean summary message to user {user_id}")
                    try:
                        await context.bot.send_message(
                            chat_id=user_id,
                            text=message,
                            parse_mode=ParseMode.MARKDOWN
                        )
                        await asyncio.sleep(0.3)
                    except aiohttp.ClientError as e:
                        logger.error(f"å‘é€æ¶ˆæ¯åˆ° {user_id} å¤±è´¥: {str(e)}")
    except Exception as e:
        error_msg = f"âŒ è‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {str(e)}"
        for user_id in config.ALLOWED_USER_IDS:
            if context.bot_data.get(f"notify_clean_{user_id}", False):
                logger.info(f"Sending auto-clean error message to user {user_id}")
                try:
                    await context.bot.send_message(
                        chat_id=user_id,
                        text=error_msg,
                        parse_mode=ParseMode.MARKDOWN
                    )
                    await asyncio.sleep(0.3)
                except aiohttp.ClientError as e:
                    logger.error(f"å‘é€æ¶ˆæ¯åˆ° {user_id} å¤±è´¥: {str(e)}")

async def find_download_directory(token: str, base_url: str, parent_dir: str, original_code: str) -> Tuple[Optional[List[str]], Optional[str]]:
    """æŸ¥æ‰¾åŒ…å«ç•ªå·çš„ç›®å½•"""
    logger.info(f"æœç´¢ç•ªå· '{original_code}'")
    list_url = f"{base_url.rstrip('/')}/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    try:
        parent_dir = parent_dir.strip().replace('\\', '/').rstrip('/')
        if not parent_dir.startswith('/'):
            parent_dir = f'/{parent_dir}'
        list_payload = {"path": parent_dir, "page": 1, "per_page": 0}
        async with aiohttp.ClientSession() as session:
            async with session.post(list_url, json=list_payload, headers=headers, timeout=20) as response:
                response.raise_for_status()
                list_result = await response.json()
                if list_result.get("code") != 200:
                    return None, f"ç›®å½•åˆ—è¡¨å¤±è´¥: {list_result.get('message', 'æœªçŸ¥é”™è¯¯')}"
                content = list_result.get("data", {}).get("content", [])
                target_pattern = re.sub(r'[^a-zA-Z0-9]', '', original_code).lower()
                possible_matches = []
                for item in content:
                    if item.get("is_dir"):
                        dir_name = item.get("name", "").strip()
                        normalized_dir = re.sub(r'[^a-zA-Z0-9]', '', dir_name).lower()
                        if normalized_dir.startswith(target_pattern):
                            full_path = f"{parent_dir.rstrip('/')}/{dir_name}".replace('//', '/')
                            possible_matches.append(full_path)
                return possible_matches, None
    except aiohttp.ClientError as e:
        logger.error(f"ç›®å½•æœç´¢å¤±è´¥: {str(e)}")
        return None, "ç›®å½•æœç´¢å¤±è´¥"

def extract_jav_prefix(name: str) -> Optional[str]:
    """æå– JAV å‰ç¼€"""
    logger.debug(f"åŸå§‹åç§°: {name}")
    
    # æ¸…ç†åç§°ï¼Œä¿ç•™å…³é”®ç•ªå·éƒ¨åˆ†
    clean_name = re.sub(
        r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA70-\U0001FAFF\U00002702-\U000027B0\U00002600-\U000026FF]+',  # Emoji
        '', name, flags=re.UNICODE)
    clean_name = re.sub(
        r'(?:[Cc]hinese|[Ee]nglish)\s*[Ss]ubtitles?|ä¸­æ–‡å­—å¹•|è‹±æ–‡å­—å¹•|é«˜æ¸…|æ— ç |ç„¡ç¢¼|æ— æ°´å°|ç„¡æ°´å°|ç ´è§£ç‰ˆ|ç ´è§£|[\(\[]\d{4}[\)\]]|'  # å¹´ä»½æ‹¬å·
        r'FHD|HD|SD|1080p|720p|4K|Blu[Rr]ay|WEB-DL|REPACK|UNCENSORED|LEAKED|'  # è´¨é‡æ ‡ç­¾
        r'\[.*?\]|ã€.*?ã€‘|\(.*?\)',  # å„ç§æ‹¬å·å†…å®¹
        '',
        clean_name,
        flags=re.IGNORECASE
    ).strip()
    clean_name = re.sub(r'\s+', ' ', clean_name)  # åˆå¹¶å¤šä½™ç©ºæ ¼
    logger.debug(f"æ¸…ç†ååç§°: {clean_name}")

    # ä¼˜å…ˆåŒ¹é…æ›´æ˜ç¡®çš„æ¨¡å¼
    patterns = [
        # FC2-PPV-æ•°å­—(6+): FC2-PPV-1234567, FC2PPV123456
        r'(FC2)[-_]?(PPV)[-_]?(\d{6,})',
        # FC2-æ•°å­—(6+): FC2-1234567, FC2123456 (éPPV)
        r'(FC2)[-_]?(\d{6,})',
        # å­—æ¯(2-5)-æ•°å­—(2-7)ï¼Œå¯é€‰å­—æ¯åç¼€: ABC-123, DEFG-1001, HIJKL-500A
        r'([A-Z]{2,5})[-_ ]?(\d{2,7})(?:[-_ ]?([A-Z]+))?',
        # é•¿å­—æ¯ä¸²+æ•°å­—(3+): HEYZO123, CARIBBEAN001
        r'([A-Z]{4,})(\d{3,})'
    ]

    for pattern in patterns:
        match = re.search(pattern, clean_name, re.IGNORECASE)
        if match:
            groups = match.groups()
            prefix = groups[0].upper()
            logger.debug(f"åŒ¹é…å‰ç¼€: {prefix}, å®Œæ•´åŒ¹é…: {match.group(0)}")
            # ç‰¹æ®Šå¤„ç† FC2
            if prefix == 'FC2':
                # å¦‚æœåŒ¹é…åˆ° FC2-PPV æ¨¡å¼ï¼Œæˆ–åç§°åŒ…å« PPV
                if len(groups) > 1 and groups[1] and groups[1].upper() == 'PPV':
                    return 'FC2-PPV'
                elif 'PPV' in name.upper():
                    return 'FC2-PPV'
                else:
                    return 'FC2'
            # å¯¹äºå…¶ä»–å‰ç¼€ï¼Œç›´æ¥è¿”å›
            return prefix

    logger.trace(f"æ— æ³•ä»åç§° '{name}' (æ¸…ç†å: '{clean_name}') æå– JAV å‰ç¼€")
    return None

def get_destination_subdir(name: str, custom_categories: List[Dict[str, List[str]]]) -> str:
    """ç¡®å®šæ–‡ä»¶çš„åˆ†ç±»å­ç›®å½•"""
    logger.debug(f"ç¡®å®šå­ç›®å½•ï¼Œåç§°: {name}")
    prefix = extract_jav_prefix(name)
    logger.debug(f"æå–çš„å‰ç¼€: {prefix}")
    if prefix:
        if prefix.startswith('FC2') or prefix == 'FC2-PPV':
            logger.debug("åˆ†ç±»åˆ°: JAV/FC2")
            return 'JAV/FC2'
        first_letter = prefix[0].upper()
        logger.debug(f"åˆ†ç±»åˆ°: JAV/{first_letter}")
        return f"JAV/{first_letter}"
    name_lower = name.lower()
    for category in custom_categories:
        if any(kw in name_lower for kw in category["keywords"]):
            logger.debug(f"åˆ†ç±»åˆ°: {category['name']}")
            return category["name"]
    logger.debug("åˆ†ç±»åˆ°: å…¶ä»–")
    return "å…¶ä»–"

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type(aiohttp.ClientError)
)
async def list_directory(token: str, base_url: str, path: str) -> List[Dict]:
    """åˆ—å‡ºç›®å½•å†…å®¹ï¼Œä¿è¯å§‹ç»ˆè¿”å›åˆ—è¡¨"""
    url = f"{base_url.rstrip('/')}/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    payload = {"path": path, "page": 1, "per_page": 0}
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload, headers=headers, timeout=15) as response:
                # å¤„ç†é200çŠ¶æ€ç 
                if response.status != 200:
                    logger.error(f"ç›®å½•åˆ—è¡¨å¤±è´¥ HTTPçŠ¶æ€ç : {response.status}")
                    return []
                
                result = await response.json()
                if result.get("code") == 200:
                    return result.get("data", {}).get("content", [])
                
                logger.error(f"ç›®å½•åˆ—è¡¨å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return []
    
    except Exception as e:
        logger.error(f"è·å–ç›®å½•å†…å®¹å¤±è´¥: {str(e)}")
        return []  # ç¡®ä¿è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯ None

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(aiohttp.ClientError)
)
async def create_directory_recursive(token: str, path: str) -> bool:
    """é€’å½’åˆ›å»ºç›®å½•ï¼Œå®Œå…¨å¤„ç†è·¯å¾„å¼‚å¸¸"""
    try:
        # è§„èŒƒåŒ–è·¯å¾„å¤„ç†
        path = path.strip().replace('\\', '/').rstrip('/')
        
        # å¤„ç†æ ¹ç›®å½•å’Œç©ºè·¯å¾„
        if not path:
            logger.error("åˆ›å»ºç›®å½•å¤±è´¥: ç©ºè·¯å¾„")
            return False
        if path == "/":
            logger.debug("æ ¹ç›®å½•å·²å­˜åœ¨")
            return True
        
        # åˆ†å‰²è·¯å¾„ä¸ºå±‚çº§ç»“æ„
        parts = [p for p in path.lstrip('/').split('/') if p.strip()]
        if not parts:
            logger.error("æ— æ•ˆè·¯å¾„æ ¼å¼")
            return False
        
        current_path = ""
        for part in parts:
            try:
                # æ„å»ºå½“å‰è·¯å¾„
                current_path = f"{current_path}/{part}".lstrip('/')
                full_path = f"/{current_path}"
                
                # è·³è¿‡ç©ºè·¯å¾„æ®µ
                if not part.strip():
                    continue
                
                # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                if not await directory_exists(token, full_path):
                    # åˆ›å»ºç›®å½•
                    mkdir_url = f"{config.BASE_URL.rstrip('/')}/api/fs/mkdir"
                    headers = {"Authorization": token}
                    data = {"path": full_path}
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.post(mkdir_url, json=data, headers=headers, timeout=15) as response:
                            if response.status != 200:
                                logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥ HTTP {response.status}: {full_path}")
                                return False
                            
                            result = await response.json()
                            if result.get("code") != 200:
                                logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥: {result.get('message')} ({full_path})")
                                return False
                            
                            logger.debug(f"å·²åˆ›å»ºç›®å½•: {full_path}")
            
            except Exception as e:
                logger.error(f"åˆ›å»ºç›®å½• {full_path} æ—¶å‡ºé”™: {str(e)}")
                return False
        
        return True
    
    except Exception as e:
        logger.error(f"é€’å½’åˆ›å»ºç›®å½•å¼‚å¸¸: {str(e)}")
        return False

async def directory_exists(token: str, path: str) -> bool:
    """æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨"""
    parent, name = os.path.dirname(path), os.path.basename(path)
    
    # è·å–ç›®å½•å†…å®¹æ—¶å¤„ç†å¯èƒ½çš„ None è¿”å›å€¼
    content = await list_directory(token, config.BASE_URL, parent)
    
    # å¤„ç† content ä¸º None çš„æƒ…å†µ
    if content is None:
        logger.warning(f"ç›®å½• {parent} åˆ—è¡¨è¿”å›ç©ºç»“æœ")
        return False
    
    # éå†å†…å®¹æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    for item in content:
        if item.get("name") == name and item.get("is_dir"):
            return True
    return False

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10),
    retry=retry_if_exception_type((aiohttp.ClientError, RetryAfter))
)
async def ensure_directory_exists(token: str, path: str) -> Tuple[bool, str]:
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œé€’å½’åˆ›å»ºæ‰€æœ‰çˆ¶ç›®å½•"""
    try:
        # è°ƒç”¨é€’å½’åˆ›å»ºå‡½æ•°
        success = await create_directory_recursive(token, path)
        if success:
            return True, ""
        else:
            return False, "æ— æ³•åˆ›å»ºç›®å½•ç»“æ„ï¼Œè¯·æ£€æŸ¥è·¯å¾„æƒé™æˆ–ç½‘ç»œè¿æ¥"
    except Exception as e:
        logger.error(f"åˆ›å»ºç›®å½• {path} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}")
        return False, f"ç›®å½•åˆ›å»ºå¤±è´¥: {str(e)}"

async def move_items(token: str, src_dir: str, names: List[str], dst_dir: str) -> Tuple[bool, str]:
    dir_exists, error = await ensure_directory_exists(token, dst_dir)
    if not dir_exists:
        return False, f"ç›®å½•å‡†å¤‡å¤±è´¥: {dst_dir} - {error}"
    
    url = f"{config.BASE_URL.rstrip('/')}/api/fs/move"
    headers = {"Authorization": token}
    data = {
        "src_dir": normalize_path(src_dir),
        "dst_dir": normalize_path(dst_dir),
        "names": names
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=data, headers=headers, timeout=30) as resp:
                result = await resp.json()
                if resp.status == 200 and result.get("code") == 200:
                    return True, ""
                return False, result.get("message", "ç§»åŠ¨å¤±è´¥")
    except Exception as e:
        return False, f"ç½‘ç»œé”™è¯¯: {str(e)}"

async def count_items_to_move(token: str, target_dir: str, classify_type: str, top_dirs: List[str]) -> int:
    """ç»Ÿè®¡éœ€è¦ç§»åŠ¨çš„é¡¹ç›®æ€»æ•°"""
    content = await list_directory(token, config.BASE_URL, target_dir) or []
    count = 0
    for item in content:
        if should_process_item(item, classify_type, top_dirs, target_dir):
            count += 1
    logger.debug(f"ç›®å½• {target_dir} éœ€ç§»åŠ¨é¡¹ç›®æ•°: {count}")
    return count

async def process_classification(token: str, target_dir: str, classify_type: str, moved_items: List[int], lock: asyncio.Lock, error_messages: List[str], update_progress: Callable) -> None:
    """ä¼˜åŒ–ç‰ˆåˆ†ç±»é€»è¾‘ï¼Œå®æ—¶æ›´æ–°ç§»åŠ¨è¿›åº¦"""
    limiter = AsyncLimiter(2, 1)

    top_dirs = ["JAV"] + [cat["name"] for cat in config.CUSTOM_CATEGORIES] + ["å…¶ä»–"]
    for dir_name in top_dirs:
        dest_path = f"{target_dir}/{dir_name}"
        success, error = await ensure_directory_exists(token, dest_path)
        if not success:
            error_messages.append(f"ç›®å½•å‡†å¤‡å¤±è´¥ {dir_name}: {error}")
        # é¢å¤–ç¡®ä¿ JAV/FC2 ç›®å½•
        if dir_name == "JAV":
            fc2_path = f"{target_dir}/JAV/FC2"
            success, error = await ensure_directory_exists(token, fc2_path)
            if not success:
                error_messages.append(f"ç›®å½•å‡†å¤‡å¤±è´¥ JAV/FC2: {error}")

    content = await list_directory(token, config.BASE_URL, target_dir) or []
    move_map = defaultdict(list)
    
    for item in content:
        if not should_process_item(item, classify_type, top_dirs, target_dir):
            continue
        dest = get_destination_subdir(item["name"], config.CUSTOM_CATEGORIES)
        move_map[f"{target_dir}/{dest}"].append(item["name"])

    logger.debug(f"ç§»åŠ¨æ˜ å°„: {dict(move_map)}")
    
    for dest_path, names in move_map.items():
        for i in range(0, len(names), 5):
            batch = names[i:i+5]
            async with limiter:
                success, error = await move_items(token, target_dir, batch, dest_path)
                if success:
                    async with lock:
                        moved_items[0] += len(batch)
                    await update_progress()
                    logger.debug(f"ç§»åŠ¨ {len(batch)} ä¸ªé¡¹ç›®åˆ° {dest_path}")
                else:
                    error_messages.append(f"ç§»åŠ¨å¤±è´¥åˆ° {os.path.basename(dest_path)}: {error}")
                    if "object not found" in error.lower():
                        await ensure_directory_exists(token, dest_path)
                        success, error = await move_items(token, target_dir, batch, dest_path)
                        if success:
                            async with lock:
                                moved_items[0] += len(batch)
                            await update_progress()
                            error_messages.pop()
                            logger.debug(f"ä¿®å¤åæˆåŠŸç§»åŠ¨ {len(batch)} ä¸ªé¡¹ç›®åˆ° {dest_path}")

def should_process_item(item: dict, classify_type: str, top_dirs: List[str], target_dir: str) -> bool:
    name = item.get("name", "")
    is_dir = item.get("is_dir", False)
    
    if classify_type == "folder" and not is_dir:
        return False
    if classify_type == "file" and is_dir:
        return False
    if is_system_path(f"{target_dir}/{name}", config.SYSTEM_FOLDERS):
        return False
    if name in top_dirs:
        return False
    return True

async def resolve_target_dirs(token: str, base_dir: str, target: Optional[str]) -> List[str]:
    if not target or target == "/":
        return [base_dir]
    
    if FANHAO_REGEX.match(target):
        dirs, _ = await find_download_directory(token, config.BASE_URL, base_dir, target)
        return dirs or []
    
    target_path = normalize_path(os.path.join(base_dir, target))
    if await directory_exists(token, target_path):
        return [target_path]
    
    return []

@restricted
async def classify_command(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """æ”¯æŒæ‰¹é‡æ“ä½œçš„åˆ†ç±»å‘½ä»¤ï¼Œä¼˜åŒ–è¿›åº¦æ¡æ˜¾ç¤º"""
    args = context.args or []
    classify_type = "all"
    target = None
    
    if args:
        first_arg = args[0].lower()
        if first_arg in ("all", "folder", "file"):
            classify_type = first_arg
            target = args[1] if len(args) > 1 else None
        else:
            target = first_arg
            if len(args) > 1 and args[1].lower() in ("all", "folder", "file"):
                classify_type = args[1].lower()

    current_dir = context.bot_data.get('current_download_dir', config.ALIST_OFFLINE_DIRS[0])
    target_dirs = await resolve_target_dirs(token, current_dir, target)
    if not target_dirs:
        await update.effective_message.reply_text("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆç›®æ ‡ç›®å½•")
        return

    top_dirs = ["JAV"] + [cat["name"] for cat in config.CUSTOM_CATEGORIES] + ["å…¶ä»–"]
    total_items = 0
    for dir_path in target_dirs:
        total_items += await count_items_to_move(token, dir_path, classify_type, top_dirs)
    logger.info(f"å…±éœ€ç§»åŠ¨ {total_items} ä¸ªé¡¹ç›®ï¼Œç›®æ ‡ç›®å½•æ•°: {len(target_dirs)}")

    progress_msg = await update.effective_message.reply_text(
        f"ğŸ”„ å¼€å§‹æ‰¹é‡æ•´ç†ã€{classify_type}ã€‘ç±»å‹ï¼Œå…± {total_items} ä¸ªé¡¹ç›®...\n"
        f"â–°â–±â–±â–±â–±â–±â–±â–±â–± 0%"
    )

    semaphore = asyncio.Semaphore(15)
    moved_items = [0]
    error_messages = []
    lock = asyncio.Lock()

    async def update_progress():
        progress = int((moved_items[0] / total_items) * 100) if total_items > 0 else 100
        progress_bar = "â–°" * (progress // 5) + "â–±" * (20 - progress // 5)
        await safe_edit_message(
            progress_msg,
            f"ğŸ”„ æ‰¹é‡æ•´ç†ä¸­...\n"
            f"{progress_bar} {progress}%\n"
            f"â€¢ å·²ç§»åŠ¨: {moved_items[0]}/{total_items} é¡¹ç›®\n"
            f"â€¢ é”™è¯¯: {len(error_messages)}"
        )

    async def process_with_semaphore(dir_path):
        async with semaphore:
            await process_classification(token, dir_path, classify_type, moved_items, lock, error_messages, update_progress)

    tasks = [process_with_semaphore(dir_path) for dir_path in target_dirs]
    await asyncio.gather(*tasks, return_exceptions=True)

    result_msg = [
        f"âœ… æ•´ç†å®Œæˆï¼",
        f"â€¢ å¤„ç†ç›®å½•: {len(target_dirs)}",
        f"â€¢ ç§»åŠ¨é¡¹ç›®: {moved_items[0]}",
        f"â€¢ å¤±è´¥é¡¹ç›®: {len(error_messages)}"
    ]
    
    if error_messages:
        error_samples = "\n".join([f"â€¢ {e}" for e in error_messages[:3]])
        result_msg.append(f"\né”™è¯¯ç¤ºä¾‹:\n{error_samples}")
        if len(error_messages) > 3:
            result_msg.append("...ï¼ˆæ›´å¤šé”™è¯¯è¯¦è§æ—¥å¿—ï¼‰")

    user_id = update.effective_user.id
    if context.bot_data.get(f"notify_task_{user_id}", False):
        await update.effective_message.reply_text(
            f"âœ… åˆ†ç±»ä»»åŠ¡å®Œæˆ: {'/'.join(target_dirs)}",
            parse_mode=ParseMode.MARKDOWN
        )
    await safe_edit_message(progress_msg, "\n".join(result_msg))
    await asyncio.sleep(3)
    await refresh_command(update, context)

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    user_id = update.effective_user.id
    if user_id not in config.ALLOWED_USER_IDS:
        await update.effective_message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    help_text = (
        f'### JAV ä¸‹è½½æœºå™¨äºº {BOT_VERSION}\n'
        '1. å‘é€ç•ªå·ï¼ˆå¦‚ `ABC-123` æˆ– `FC2-PPV-1010519`ï¼‰\n'
        '2. å‘é€ç£åŠ›é“¾æ¥ï¼ˆ`magnet:?`ï¼‰æˆ– ed2k é“¾æ¥ï¼ˆ`ed2k://`ï¼‰\n'
        '3. **æ¸…ç†åŠŸèƒ½**ï¼š\n'
        '   - `/clean` æ¸…ç†å½“å‰ç›®å½•\n'
        '   - `/clean <ç•ªå·>` æ¸…ç†ç‰¹å®šç›®å½•\n'
        '4. **ç›®å½•ç®¡ç†**ï¼š\n'
        '   - `/setdir` åˆ—å‡ºå¹¶é€‰æ‹©ä¸‹è½½ç›®å½•\n'
        '   - `/reload_config` é‡æ–°åŠ è½½é…ç½®\n'
        '5. **åˆ†ç±»æ•´ç†**:\n'
        '   - `/classify [all|folder|file]` æ™ºèƒ½æ•´ç†\n'
        '     ç¤ºä¾‹:\n'
        '     /classify       - æ•´ç†æ‰€æœ‰ç±»å‹\n'
        '     /classify folder - ä»…æ•´ç†æ–‡ä»¶å¤¹\n'
        '     /classify file   - ä»…æ•´ç†æ–‡ä»¶\n'
        '6. **åˆ·æ–°alist**:\n'
        '   - `/refresh`\n'
        '7. **é€šçŸ¥è®¾ç½®**:\n'
        '   - `/notify æ¸…ç† <on|off>` è®¾ç½®é€šçŸ¥\n'
        '     ç¤ºä¾‹:\n'
        '     /notify æ¸…ç† off - å…³é—­è‡ªåŠ¨æ¸…ç†é€šçŸ¥\n'
        f'**å½“å‰ç›®å½•**ï¼š`{context.bot_data.get("current_download_dir", "æœªçŸ¥")}`'
    )
    await update.effective_message.reply_text(help_text, parse_mode=ParseMode.MARKDOWN)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¯åŠ¨å‘½ä»¤"""
    user_id = update.effective_user.id
    if user_id not in config.ALLOWED_USER_IDS:
        await update.effective_message.reply_text("æŠ±æ­‰ï¼Œæ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    keyboard = [[InlineKeyboardButton("æŸ¥çœ‹å¸®åŠ©", callback_data='help')]]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.effective_message.reply_text(
        'æ¬¢è¿ä½¿ç”¨ JAV ä¸‹è½½æœºå™¨äººï¼\n'
        'å‘é€ç•ªå·æˆ–ç£åŠ›/ed2k é“¾æ¥ä»¥æ·»åŠ åˆ° Alistã€‚\n'
        'ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®æŸ¥çœ‹å¸®åŠ©ã€‚',
        reply_markup=reply_markup
    )

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç†æŒ‰é’®å›è°ƒ"""
    query = update.callback_query
    await query.answer()
    if query.data == 'help':
        await help_command(update, context)

async def handle_single_entry(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str, entry: str):
    """å¤„ç†å•æ¡è¾“å…¥"""
    chat_id = update.effective_chat.id
    processing_msg = None
    try:
        if entry.startswith(("magnet:?", "ed2k://")):
            processing_msg = await update.effective_message.reply_text(f"ğŸ”— æ”¶åˆ°{'ç£åŠ›' if entry.startswith('magnet:?') else 'ed2k'}é“¾æ¥")
            success, result_msg = await add_offline_download(context, token, [entry])
            await processing_msg.edit_text(result_msg)
            if success:
                user_id = update.effective_user.id
                if context.bot_data.get(f"notify_task_{user_id}", False):
                    await update.effective_message.reply_text(
                        f"âœ… ä»»åŠ¡å®Œæˆï¼š{entry}",
                        parse_mode=ParseMode.MARKDOWN
                    )
                await asyncio.sleep(3)
                await refresh_command(update, context)
        elif FANHAO_REGEX.match(entry):
            processing_msg = await update.effective_message.reply_text(f"ğŸ” æœç´¢ç•ªå·: {entry}")
            await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
            magnet, error_msg = await search_magnet(entry, config.SEARCH_URLS, context)
            if not magnet:
                await processing_msg.edit_text(f"âŒ æœç´¢å¤±è´¥: {error_msg}")
                return
            await processing_msg.edit_text(f"âœ… æ‰¾åˆ°ç£åŠ›é“¾æ¥")
            success, add_msg = await add_offline_download(context, token, [magnet])
            await processing_msg.edit_text(add_msg)
            if success:
                user_id = update.effective_user.id
                if context.bot_data.get(f"notify_task_{user_id}", False):
                    await update.effective_message.reply_text(
                        f"âœ… ä»»åŠ¡å®Œæˆï¼š{entry}",
                        parse_mode=ParseMode.MARKDOWN
                    )
                await asyncio.sleep(3)
                await refresh_command(update, context)
        else:
            await update.effective_message.reply_text("æ— æ³•è¯†åˆ«çš„æ ¼å¼")
    except aiohttp.ClientError as e:
        error_msg = "âŒ å¤„ç†å¤±è´¥"
        if processing_msg:
            await processing_msg.edit_text(error_msg)
        else:
            await update.effective_message.reply_text(error_msg)

async def safe_edit_message(progress_msg, text, retry_count=3):
    """å®‰å…¨ç¼–è¾‘æ¶ˆæ¯"""
    for attempt in range(retry_count):
        try:
            await progress_msg.edit_text(text)
            return True
        except RetryAfter as e:
            await asyncio.sleep(e.retry_after)
        except aiohttp.ClientError as e:
            logger.error(f"ç¼–è¾‘æ¶ˆæ¯å¤±è´¥: {str(e)}")
            return False
    return False

@restricted
async def handle_batch_entries(update: Update, context: ContextTypes.DEFAULT_TYPE, entries: List[str], *, token: str):
    """å¤„ç†æ‰¹é‡è¾“å…¥"""
    chat_id = update.effective_chat.id
    total_tasks = len(entries)
    progress_msg = await update.effective_message.reply_text(f"ğŸ”„ å¤„ç† {total_tasks} ä¸ªä»»åŠ¡")
    try:
        task_results = []
        all_urls = []
        seen_magnets = set()
        success_count = 0
        update_interval = max(1, total_tasks // 5)
        for idx, entry in enumerate(entries, 1):
            if entry.startswith(("magnet:?", "ed2k://")):
                if entry not in seen_magnets:
                    all_urls.append((entry, entry))
                    seen_magnets.add(entry)
                    task_results.append(f"{idx}. {entry[:15]}...: å¾…æ·»åŠ ")
                else:
                    task_results.append(f"{idx}. {entry[:15]}...: é‡å¤")
            elif FANHAO_REGEX.match(entry):
                magnet, error_msg = await search_magnet(entry, config.SEARCH_URLS, context)
                if magnet and magnet not in seen_magnets:
                    all_urls.append((magnet, entry))
                    seen_magnets.add(magnet)
                    task_results.append(f"{idx}. {entry}: å¾…æ·»åŠ ")
                else:
                    task_results.append(f"{idx}. {entry}: âŒæ— èµ„æº")
            else:
                task_results.append(f"{idx}. {entry[:15]}...: æ— æ•ˆ")
            if idx % update_interval == 0 or idx == total_tasks:
                await safe_edit_message(progress_msg, f"â³ å¤„ç† {idx}/{total_tasks}\n" + "\n".join(task_results[-5:]))

        url_total = len(all_urls)
        for url_idx, (url, original_entry) in enumerate(all_urls, 1):
            success, msg = await add_offline_download(context, token, [url])
            for idx, entry in enumerate(entries, 1):
                if entry == original_entry or (FANHAO_REGEX.match(entry) and entry in original_entry):
                    if success:
                        task_results[idx-1] = f"{idx}. {original_entry[:15]}...: âœ…æˆåŠŸ"
                        success_count += 1
                        user_id = update.effective_user.id
                        if context.bot_data.get(f"notify_task_{user_id}", False):
                            await update.effective_message.reply_text(
                                f"âœ… ä»»åŠ¡å®Œæˆï¼š{original_entry}",
                                parse_mode=ParseMode.MARKDOWN
                            )
                    else:
                        task_results[idx-1] = f"{idx}. {original_entry[:15]}...: {msg}"
                    break
            if url_idx % update_interval == 0 or url_idx == url_total:
                await safe_edit_message(progress_msg, f"â³ æ·»åŠ  {url_idx}/{url_total}\næˆåŠŸ: {success_count}")

        summary = f"âœ… å®Œæˆ (æˆåŠŸ: {success_count}/{total_tasks})\n" + "\n".join(task_results[:10])
        if len(task_results) > 10:
            summary += f"\n...ï¼ˆå…± {len(task_results)} æ¡ï¼‰"
        await safe_edit_message(progress_msg, summary)
        if success_count > 0:
            await asyncio.sleep(3)
            await refresh_command(update, context)
    except aiohttp.ClientError as e:
        error_msg = f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥\n" + "\n".join(task_results[:5])
        await safe_edit_message(progress_msg, error_msg)

@restricted
async def process_message(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
    message_text = update.message.text.strip()
    entries = [line.strip() for line in message_text.split('\n') if line.strip()]
    if not entries:
        await update.effective_message.reply_text("âš ï¸ è¾“å…¥ä¸ºç©º")
        return
    if len(entries) == 1:
        await handle_single_entry(update, context, token, entries[0])
    else:
        await handle_batch_entries(update, context, entries)

@restricted
async def clean_command(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """æ¸…ç†å°æ–‡ä»¶å’Œç©ºç›®å½•"""
    if config.SIZE_THRESHOLD == 0:
        await update.effective_message.reply_text("âœ… æ¸…ç†åŠŸèƒ½å·²ç¦ç”¨")
        return

    target = context.args[0].strip() if context.args else None
    chat_id = update.effective_chat.id
    target_display = target if target else "å½“å‰ç›®å½•"
    processing_msg = await update.effective_message.reply_text(f"ğŸ§¹ æ¸…ç†ä»»åŠ¡ï¼ˆç›®æ ‡: {target_display}ï¼‰")

    try:
        current_dir = context.bot_data.get('current_download_dir', config.ALIST_OFFLINE_DIRS[0])
        results = []
        total_files = 0
        total_dirs_cleaned = 0

        # ç¡®å®šç›®æ ‡ç›®å½•
        if not target:
            target_dirs = [current_dir]
        else:
            directories, find_error = await find_download_directory(token, config.BASE_URL, current_dir, target)
            if not directories:
                await processing_msg.edit_text(f"âŒ æ¸…ç†å¤±è´¥: {find_error}")
                return
            target_dirs = directories

        # ç»Ÿè®¡å°æ–‡ä»¶æ€»æ•°
        total_small_files = 0
        for target_dir in target_dirs:
            dir_files, _ = await recursive_collect_files(token, config.BASE_URL, target_dir)
            total_small_files += sum(len(files) for files in dir_files.values())

        deleted_small_files = [0]
        update_interval = max(10, total_small_files // 10) if total_small_files > 0 else 1

        async def update_small_files_progress():
            if deleted_small_files[0] % update_interval == 0 or deleted_small_files[0] == total_small_files:
                progress = int((deleted_small_files[0] / total_small_files) * 100) if total_small_files > 0 else 100
                progress_bar = "â–°" * (progress // 10) + "â–±" * (10 - progress // 10)
                await safe_edit_message(
                    processing_msg,
                    f"ğŸ§¹ æ¸…ç†å°æ–‡ä»¶: {progress}% [{progress_bar}]"
                )

        def small_files_progress_callback(deleted: int):
            deleted_small_files[0] += deleted
            asyncio.create_task(update_small_files_progress())

        # æ¸…ç†å°æ–‡ä»¶
        for target_dir in target_dirs:
            deleted, msg = await cleanup_small_files(
                token, config.BASE_URL, target_dir, progress_callback=small_files_progress_callback
            )
            total_files += deleted
            results.append(f"æ¸…ç† {target_dir}:\n- {msg}")

        # æ¸…ç†ç©ºæ–‡ä»¶å¤¹
        empty_dirs = await recursive_collect_empty_dirs(token, config.BASE_URL, current_dir)
        total_empty_dirs = len([d for d in empty_dirs if not is_system_path(d, config.SYSTEM_FOLDERS)])
        deleted_empty_dirs = [0]
        update_interval_empty = max(5, total_empty_dirs // 10) if total_empty_dirs > 0 else 1

        async def update_empty_dirs_progress():
            if deleted_empty_dirs[0] % update_interval_empty == 0 or deleted_empty_dirs[0] == total_empty_dirs:
                progress = int((deleted_empty_dirs[0] / total_empty_dirs) * 100) if total_empty_dirs > 0 else 100
                progress_bar = "â–°" * (progress // 10) + "â–±" * (10 - progress // 10)
                await safe_edit_message(
                    processing_msg,
                    f"ğŸ§¹ æ¸…ç†ç©ºæ–‡ä»¶å¤¹: {progress}% [{progress_bar}]"
                )

        def empty_dirs_progress_callback(deleted: int):
            deleted_empty_dirs[0] += deleted
            asyncio.create_task(update_empty_dirs_progress())

        if total_empty_dirs > 0:
            await safe_edit_message(processing_msg, f"ğŸ§¹ æ¸…ç†ç©ºæ–‡ä»¶å¤¹: 0% [â–±â–±â–±â–±â–±â–±â–±â–±â–±â–±]")
            deleted_dirs, msg_dirs = await cleanup_empty_dirs(
                token, config.BASE_URL, current_dir, progress_callback=empty_dirs_progress_callback
            )
            total_dirs_cleaned += deleted_dirs
            results.append(f"æ¸…ç†ç©ºæ–‡ä»¶å¤¹ {current_dir}:\n- {msg_dirs}")
        else:
            results.append(f"æ¸…ç†ç©ºæ–‡ä»¶å¤¹ {current_dir}:\n- âœ… æœªæ‰¾åˆ°ç©ºæ–‡ä»¶å¤¹")

        await safe_edit_message(processing_msg, "\n\n".join(results))
        await asyncio.sleep(3)
        await refresh_command(update, context)
    except aiohttp.ClientError as e:
        error_msg = f"âŒ æ¸…ç†å¤±è´¥: {str(e)}"
        await processing_msg.edit_text(error_msg)

@restricted
async def refresh_command(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """åˆ·æ–°Alistæ–‡ä»¶åˆ—è¡¨"""
    refresh_url = f"{config.BASE_URL.rstrip('/')}/api/fs/list"
    headers = {"Authorization": token, "Content-Type": "application/json"}
    payload = {"path": context.bot_data.get('current_download_dir', config.ALIST_OFFLINE_DIRS[0]), "page": 1, "per_page": 0, "refresh": True}
    chat_id = update.effective_chat.id
    processing_msg = await update.effective_message.reply_text("ğŸ”„ æ­£åœ¨åˆ·æ–° Alist æ–‡ä»¶åˆ—è¡¨...")
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(refresh_url, json=payload, headers=headers, timeout=30) as response:
                response.raise_for_status()
                result = await response.json()
                if result.get("code") == 200:
                    await processing_msg.edit_text("âœ… Alist åˆ·æ–°æˆåŠŸï¼")
                else:
                    error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                    await processing_msg.edit_text(f"âŒ åˆ·æ–°å¤±è´¥: {error_msg}")
    except aiohttp.ClientError as e:
        logger.error(f"Alist åˆ·æ–°é”™è¯¯: {str(e)}")
        error_msg = "âŒ åˆ·æ–°å¤±è´¥"
        await processing_msg.edit_text(error_msg)

@restricted
async def setdir_command(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """é€šè¿‡å†…è”é”®ç›˜æ˜¾ç¤ºä¸‹è½½ç›®å½•å¹¶é€‰æ‹©"""
    dirs = config.ALIST_OFFLINE_DIRS
    if not dirs:
        await update.effective_message.reply_text("æœªé…ç½®ä¸‹è½½ç›®å½•ã€‚")
        return

    # å½“å‰ç›®å½•
    current_dir = context.bot_data.get('current_download_dir', dirs[0])
    current_index = context.bot_data.get('current_download_dir_index', 0)
    
    # åˆ›å»ºå†…è”é”®ç›˜ï¼ŒæŒ‰é’®æ˜¾ç¤ºç›®å½•åç§°ï¼Œå½“å‰ç›®å½•ç”¨ ğŸ‘‰ æ ‡è®°
    keyboard = [
        [InlineKeyboardButton(f"{'ğŸ‘‰ ' if dir == current_dir else ''}{dir}", callback_data=f"setdir_{i}")]
        for i, dir in enumerate(dirs)
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)

    # å‘é€æ¶ˆæ¯ï¼Œä»…åŒ…å«æç¤º
    try:
        await update.effective_message.reply_text(
            "ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®é€‰æ‹©ç›®å½•ï¼š",
            parse_mode=ParseMode.MARKDOWN,
            reply_markup=reply_markup
        )
        logger.info(f"Sent /setdir message with {len(dirs)} directories for user {update.effective_user.id}")
    except Exception as e:
        logger.error(f"Failed to send /setdir message for user {update.effective_user.id}: {str(e)}")
        await update.effective_message.reply_text("âŒ å‘é€ç›®å½•åˆ—è¡¨å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")

async def setdir_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç† /setdir æŒ‰é’®å›è°ƒ"""
    query = update.callback_query
    try:
        await query.answer()  # å›åº”å›è°ƒ
    except Exception as e:
        logger.error(f"Failed to answer callback query for user {query.from_user.id}: {str(e)}")
        return

    logger.debug(f"Received callback query with data: {query.data} from user {query.from_user.id}")
    
    if query.data.startswith("setdir_"):
        try:
            index = int(query.data[len("setdir_"):])
            dirs = config.ALIST_OFFLINE_DIRS
            if 0 <= index < len(dirs):
                context.bot_data['current_download_dir_index'] = index
                context.bot_data['current_download_dir'] = dirs[index]
                await query.message.edit_text(
                    f"âœ… å·²åˆ‡æ¢åˆ°ç›®å½• {index + 1}: {dirs[index]}",
                    parse_mode=ParseMode.MARKDOWN
                )
                logger.info(f"User {query.from_user.id} switched to directory {dirs[index]} (index {index})")
            else:
                await query.message.edit_text(
                    f"âŒ æ— æ•ˆé€‰æ‹©ï¼šç›®å½•ç´¢å¼• {index} è¶…å‡ºèŒƒå›´",
                    parse_mode=ParseMode.MARKDOWN
                )
                logger.warning(f"Invalid directory index {index} selected by user {query.from_user.id}")
        except ValueError as e:
            await query.message.edit_text(
                "âŒ æ— æ•ˆé€‰æ‹©ï¼šæ— æ³•è§£æç›®å½•ç´¢å¼•",
                parse_mode=ParseMode.MARKDOWN
            )
            logger.error(f"Failed to parse callback data '{query.data}' for user {query.from_user.id}: {str(e)}")
        except Exception as e:
            await query.message.edit_text(
                "âŒ å¤„ç†å›è°ƒå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•",
                parse_mode=ParseMode.MARKDOWN
            )
            logger.error(f"Unexpected error in setdir_callback for user {query.from_user.id}: {str(e)}")
    else:
        await query.message.edit_text(
            "âŒ æ— æ•ˆå›è°ƒæ•°æ®",
            parse_mode=ParseMode.MARKDOWN
        )
        logger.warning(f"Unexpected callback data '{query.data}' from user {query.from_user.id}")

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç†å…¶ä»–æŒ‰é’®å›è°ƒ"""
    query = update.callback_query
    await query.answer()
    if query.data == 'help':
        await help_command(update, context)
    else:
        logger.debug(f"Ignored callback data '{query.data}' in button_callback for user {query.from_user.id}")

@restricted
async def reload_config(update: Update, context: ContextTypes.DEFAULT_TYPE, token: str) -> None:
    """çƒ­é‡æ–°åŠ è½½é…ç½®"""
    global config
    old_config = config
    new_config = Config()

    # éªŒè¯æ–°é…ç½®
    if not new_config.validate():
        await update.effective_message.reply_text("âŒ é…ç½®é‡è½½å¤±è´¥ï¼šè¯·æ£€æŸ¥ .env æ–‡ä»¶")
        return

    # æ›´æ–°å…¨å±€é…ç½®
    config = new_config
    logger.info("é…ç½®å·²çƒ­é‡è½½")

    # æ£€æŸ¥å½“å‰ä¸‹è½½ç›®å½•
    current_dir = context.bot_data.get('current_download_dir', old_config.ALIST_OFFLINE_DIRS[0])
    current_index = context.bot_data.get('current_download_dir_index', 0)
    if current_dir in config.ALIST_OFFLINE_DIRS:
        # å½“å‰ç›®å½•ä»ç„¶æœ‰æ•ˆï¼Œä¿æŒä¸å˜
        new_index = config.ALIST_OFFLINE_DIRS.index(current_dir)
        context.bot_data['current_download_dir'] = current_dir
        context.bot_data['current_download_dir_index'] = new_index
    else:
        # å½“å‰ç›®å½•ä¸å†æœ‰æ•ˆï¼Œé‡ç½®ä¸ºç¬¬ä¸€ä¸ªç›®å½•
        context.bot_data['current_download_dir'] = config.ALIST_OFFLINE_DIRS[0] if config.ALIST_OFFLINE_DIRS else ""
        context.bot_data['current_download_dir_index'] = 0

    # æ›´æ–°è‡ªåŠ¨æ¸…ç†ä»»åŠ¡
    job_queue = context.application.job_queue
    if job_queue:
        # ç§»é™¤ç°æœ‰è‡ªåŠ¨æ¸…ç†ä»»åŠ¡
        for job in job_queue.jobs():
            if job.name == "auto_clean":
                job.schedule_removal()
        
        # å¦‚æœæ¸…ç†åŠŸèƒ½å¯ç”¨ï¼Œé‡æ–°è°ƒåº¦ä»»åŠ¡
        if config.SIZE_THRESHOLD > 0 and config.ALIST_OFFLINE_DIRS:
            job_queue.run_repeating(
                auto_clean,
                interval=config.CLEAN_INTERVAL_MINUTES * 60,
                first=config.CLEAN_INTERVAL_MINUTES * 60,
                name="auto_clean"
            )
            logger.info(f"è‡ªåŠ¨æ¸…ç†ä»»åŠ¡å·²æ›´æ–°ï¼Œé—´éš” {config.CLEAN_INTERVAL_MINUTES} åˆ†é’Ÿ")

    # æ„å»ºå“åº”æ¶ˆæ¯
    message = [
        "âœ… é…ç½®å·²çƒ­é‡è½½",
        f"â€¢ å½“å‰ç›®å½•ï¼š{context.bot_data.get('current_download_dir', 'æœªçŸ¥')}",
        f"â€¢ æ¸…ç†é—´éš”ï¼š{config.CLEAN_INTERVAL_MINUTES} åˆ†é’Ÿ",
        f"â€¢ å¤§å°é˜ˆå€¼ï¼š{config.SIZE_THRESHOLD // (1024 * 1024)} MB",
        f"â€¢ ä¸‹è½½ç›®å½•æ•°ï¼š{len(config.ALIST_OFFLINE_DIRS)}",
        f"â€¢ æœç´¢APIæ•°ï¼š{len(config.SEARCH_URLS)}",
        f"â€¢ å…è®¸ç”¨æˆ·æ•°ï¼š{len(config.ALLOWED_USER_IDS)}"
    ]
    await update.effective_message.reply_text("\n".join(message), parse_mode=ParseMode.MARKDOWN)

@restricted
async def notify_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¼€å¯æˆ–å…³é—­ä»»åŠ¡å®Œæˆé€šçŸ¥å’Œè‡ªåŠ¨æ¸…ç†é€šçŸ¥"""
    user_id = update.effective_user.id
    logger.info(f"User {user_id} triggered /notify with args: {context.args}")
    if len(context.args) != 2:
        await update.effective_message.reply_text(
            "âš ï¸ æ ¼å¼ï¼š/notify <ä»»åŠ¡|æ¸…ç†> <on|off>ï¼Œä¾‹å¦‚ï¼š/notify ä»»åŠ¡ on",
            parse_mode=ParseMode.MARKDOWN
        )
        return
    notify_type = context.args[0].strip()
    state = context.args[1].lower()
    if notify_type not in ("ä»»åŠ¡", "æ¸…ç†"):
        await update.effective_message.reply_text(
            "âš ï¸ ç±»å‹å¿…é¡»ä¸º 'ä»»åŠ¡' æˆ– 'æ¸…ç†'",
            parse_mode=ParseMode.MARKDOWN
        )
        return
    if state not in ("on", "off"):
        await update.effective_message.reply_text(
            "âš ï¸ çŠ¶æ€å¿…é¡»ä¸º on æˆ– off",
            parse_mode=ParseMode.MARKDOWN
        )
        return
    key = f"notify_task_{user_id}" if notify_type == "ä»»åŠ¡" else f"notify_clean_{user_id}"
    context.bot_data[key] = (state == "on")
    logger.info(f"Set {key} to {state == 'on'} for user {user_id}")
    await update.effective_message.reply_text(
        f"âœ… {notify_type}é€šçŸ¥å·²{'å¼€å¯' if state == 'on' else 'å…³é—­'}",
        parse_mode=ParseMode.MARKDOWN
    )

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç†é”™è¯¯"""
    logger.error(f"æ›´æ–° {update} å¯¼è‡´é”™è¯¯: {context.error}")
    if update and update.effective_message:
        await update.effective_message.reply_text("âŒ å‘ç”Ÿé”™è¯¯")

def main() -> None:
    """å¯åŠ¨æœºå™¨äºº"""
    application = Application.builder().token(config.TELEGRAM_TOKEN).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("classify", classify_command))
    application.add_handler(CommandHandler("clean", clean_command))
    application.add_handler(CommandHandler("refresh", refresh_command))
    application.add_handler(CommandHandler("setdir", setdir_command))
    application.add_handler(CommandHandler("reload_config", reload_config))
    application.add_handler(CommandHandler("notify", notify_command))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, process_message))
    application.add_handler(CallbackQueryHandler(setdir_callback, pattern=r"^setdir_\d+$"))  # ä¼˜å…ˆæ³¨å†Œ setdir_callback
    application.add_handler(CallbackQueryHandler(button_callback))  # å…¶ä»–å›è°ƒå¤„ç†å™¨
    application.add_error_handler(error_handler)
    if config.SIZE_THRESHOLD > 0 and config.ALIST_OFFLINE_DIRS:
        job_queue = application.job_queue
        job_queue.run_repeating(
            auto_clean,
            interval=config.CLEAN_INTERVAL_MINUTES * 60,
            first=config.CLEAN_INTERVAL_MINUTES * 60,
            name="auto_clean"
        )
        logger.info(f"è‡ªåŠ¨æ¸…ç†ä»»åŠ¡å¯ç”¨ï¼Œé—´éš” {config.CLEAN_INTERVAL_MINUTES} åˆ†é’Ÿ")
        application.bot_data['current_download_dir'] = config.ALIST_OFFLINE_DIRS[0]
        # åˆå§‹åŒ–é»˜è®¤é€šçŸ¥çŠ¶æ€
        for user_id in config.ALLOWED_USER_IDS:
            if f"notify_task_{user_id}" not in application.bot_data:
                application.bot_data[f"notify_task_{user_id}"] = False  # é»˜è®¤å…³é—­ä»»åŠ¡é€šçŸ¥
            if f"notify_clean_{user_id}" not in application.bot_data:
                application.bot_data[f"notify_clean_{user_id}"] = True  # é»˜è®¤å¼€å¯æ¸…ç†é€šçŸ¥
    logger.info("æœºå™¨äººå¯åŠ¨...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()